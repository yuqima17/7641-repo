{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tools\n",
    "import time\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v1')\n",
    "\n",
    "gamma = 0.99\n",
    "theta = 0.000001\n",
    "def argmax(env, V, pi,s, gamma):\n",
    "    e = np.zeros(4)\n",
    "    for a in range(4):                         # iterate for every action possible \n",
    "        q=0\n",
    "        P = np.array(env.env.P[s][a])                   \n",
    "        (x,y) = np.shape(P)                             # for Bellman Equation \n",
    "        \n",
    "        for i in range(x):                              # iterate for every possible states\n",
    "            s_= int(P[i][1])                            # S' - Sprime - possible succesor states\n",
    "            p = P[i][0]                                 # Transition Probability P(s'|s,a) \n",
    "            r = P[i][2]                                 # Reward\n",
    "            \n",
    "            q += p*(r+gamma*V[s_])                      # calculate action_ value q(s|a)\n",
    "            e[a] = q\n",
    "            \n",
    "    m = np.argmax(e) \n",
    "    pi[s]=m                                           # Take index which has maximum value \n",
    "                                   # update pi(a|s) \n",
    "\n",
    "    return pi\n",
    "\n",
    "\n",
    "def bellman_optimality_update(env, V, s, gamma):  # update the stae_value V[s] by taking \n",
    "    pi = np.zeros((64))       # action which maximizes current value\n",
    "    e = np.zeros(4)                       \n",
    "                                            # STEP1: Find \n",
    "    for a in range(4):             \n",
    "        q=0                                 # iterate for all possible action\n",
    "        P = np.array(env.env.P[s][a])\n",
    "        (x,y) = np.shape(P)\n",
    "        \n",
    "        for i in range(x):\n",
    "            s_= int(P[i][1])\n",
    "            p = P[i][0]\n",
    "            r = P[i][2]\n",
    "            q += p*(r+gamma*V[s_])\n",
    "            e[a] = q\n",
    "            \n",
    "    m = np.argmax(e)\n",
    "    pi[s] = m\n",
    "    \n",
    "    value = 0\n",
    "    for a in range(4):\n",
    "        u = 0\n",
    "        P = np.array(env.env.P[s][a])\n",
    "        (x,y) = np.shape(P)\n",
    "        for i in range(x):\n",
    "            \n",
    "            s_= int(P[i][1])\n",
    "            p = P[i][0]\n",
    "            r = P[i][2]\n",
    "            \n",
    "            u += p*(r+gamma*V[s_])\n",
    "            \n",
    "        if(pi[s]==a):\n",
    "            value+=u\n",
    "            \n",
    "  \n",
    "    V[s]=value\n",
    "    return V[s]\n",
    "\n",
    "\n",
    "\n",
    "def value_iteration(env, gamma, theta):\n",
    "    start=time.time()\n",
    "    i=0\n",
    "    V = np.zeros(64)                                       # initialize v(0) to arbitory value, my case \"zeros\"\n",
    "    while True:\n",
    "        i=i+1\n",
    "        delta = 0\n",
    "        for s in range(64):                       # iterate for all states\n",
    "            v = V[s]\n",
    "            bellman_optimality_update(env, V, s, gamma)   # update state_value with bellman_optimality_update\n",
    "            delta = max(delta, abs(v - V[s]))             # assign the change in value per iteration to delta  \n",
    "        if delta < theta:                                       \n",
    "            break                                         # if change gets to negligible \n",
    "                                                          # --> converged to optimal value         \n",
    "    pi = np.zeros((64)) \n",
    "\n",
    "    for s in range(64):\n",
    "        pi = argmax(env, V, pi, s, gamma)         # extract optimal policy using action value \n",
    "    end=time.time()\n",
    "    print(\"iteration counts:\",i)\n",
    "    print(\"time:\",end-start,\"s\")\n",
    "    return V, pi\n",
    "# Naive implementation (for loops are slow), but matches the box\n",
    "def policy_iter(env, gamma, theta):\n",
    "    \"\"\"Policy Iteration Algorithm\n",
    "    \n",
    "    Params:\n",
    "        env - environment with following required memebers:\n",
    "            env.nb_states - number of states\n",
    "            env.nb_action - number of actions\n",
    "            env.model     - prob-transitions and rewards for all states and actions, see note #1\n",
    "        gamma (float) - discount factor\n",
    "        theta (float) - termination condition\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Initialization\n",
    "    i=0\n",
    "    V = np.zeros(64)\n",
    "    pi = np.zeros(64, dtype=int)  # greedy, always pick action 0\n",
    "    start=time.time()\n",
    "    while True:\n",
    "        i=i+1\n",
    "        j=0\n",
    "    \n",
    "        # 2. Policy Evaluation\n",
    "        while True:\n",
    "            j=j+1\n",
    "            delta = 0\n",
    "            for s in range(64):                       # iterate for all states\n",
    "                v = V[s]\n",
    "                bellman_optimality_update(env, V, s, gamma)   # update state_value with bellman_optimality_update\n",
    "                delta = max(delta, abs(v - V[s]))             # assign the change in value per iteration to delta  \n",
    "            if delta < theta:  \n",
    "                print(\"value iter count:\",j)\n",
    "                break       \n",
    "                \n",
    "\n",
    "        # 3. Policy Improvement\n",
    "        policy_stable = True\n",
    "        for s in range(64):\n",
    "            old_action = pi[s]\n",
    "            pi[s] = np.argmax([sum_sr(env, V=V, s=s, a=a, gamma=gamma)  # list comprehension\n",
    "                               for a in range(4)])\n",
    "            if old_action != pi[s]: policy_stable = False\n",
    "        if policy_stable: break\n",
    "    end=time.time()        \n",
    "    print(\"iteration counts:\",i)\n",
    "    print(\"time:\",end-start,\"s\")\n",
    "    return V, pi\n",
    "def sum_sr(env, V, s, a, gamma):\n",
    "    \"\"\"Calc state-action value for state 's' and action 'a'\"\"\"\n",
    "    tmp = 0  # state value for state s\n",
    "    for p, s_, r, _ in env.env.P[s][a]:     # see note #1 !\n",
    "        # p  - transition probability from (s,a) to (s')\n",
    "        # s_ - next state (s')\n",
    "        # r  - reward on transition from (s,a) to (s')\n",
    "        tmp += p * (r + gamma * V[s_])\n",
    "    return tmp\n",
    "env.render()\n",
    "\n",
    "print(\"random policy:\")\n",
    "e=0;\n",
    "for i_episode in range(100):\n",
    "    observation = env.reset()\n",
    "    for t in range(10000):\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            if reward==1:\n",
    "                e+=1\n",
    "            break\n",
    "print(e);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"value iteration:\")\n",
    "\n",
    "\n",
    "V, pi= value_iteration(env, gamma, theta)\n",
    "\n",
    "# discrete action to take in given state\n",
    "print(np.reshape(V,(8,8)))\n",
    "print(np.reshape(pi,(8,8)))\n",
    "\n",
    "e=0\n",
    "for i_episode in range(100):\n",
    "    c = env.reset()\n",
    "    for t in range(10000):\n",
    "        c, reward, done, info = env.step(pi[c])\n",
    "        if done:\n",
    "            if reward == 1:\n",
    "                e +=1\n",
    "            break\n",
    "print(e);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Policy iteration:\")\n",
    "\n",
    "\n",
    "\n",
    "V, pi = policy_iter(env, gamma, theta)\n",
    "\n",
    "\n",
    "print(np.reshape(V,(8,8)))\n",
    "print(np.reshape(pi,(8,8)))\n",
    "e=0\n",
    "for i_episode in range(100):\n",
    "    c = env.reset()\n",
    "    for t in range(10000):\n",
    "        c, reward, done, info = env.step(pi[c])\n",
    "        if done:\n",
    "            if reward == 1:\n",
    "                e +=1\n",
    "            break\n",
    "print(e)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset()\n",
    "\n",
    "\n",
    "num_observation_dimensions = np.size(observation)\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "observation_space_high = env.observation_space.high\n",
    "observation_space_low = env.observation_space.low\n",
    "observation_space_high\n",
    "num_bins_per_observation_dimension = 7 # Could try different number of bins for the different dimensions\n",
    "num_states = pow(num_bins_per_observation_dimension,num_observation_dimensions)\n",
    "\n",
    "def make_observation_bins(min, max, num_bins):\n",
    "    if(min <=-3.4e+38):\n",
    "        min = -5 # Should really learn this const instead\n",
    "    if(max >= 3.4e+38):\n",
    "        max = 5\n",
    "    print(\"min:\",min)\n",
    "    print(\"max:\",max)\n",
    "    bins = np.arange(min, max, (float(max)-float(min))/((num_bins)-2))\n",
    "    bins = np.sort(np.append(bins, [0])) # Ensure we split at 0\n",
    "    \n",
    "    return bins\n",
    "\n",
    "observation_dimension_bins = []\n",
    "for observation_dimension in range(num_observation_dimensions):\n",
    "    observation_dimension_bins.append(make_observation_bins(observation_space_low[observation_dimension], \\\n",
    "                                                            observation_space_high[observation_dimension], \\\n",
    "                                                           num_bins_per_observation_dimension))\n",
    "    \n",
    "print (observation_dimension_bins)\n",
    "def observation_to_state(observation):\n",
    "    state = 0\n",
    "    for observation_dimension in range(num_observation_dimensions):\n",
    "        state = state + np.digitize(observation[observation_dimension],observation_dimension_bins[observation_dimension]) \\\n",
    "        * pow(num_bins_per_observation_dimension, observation_dimension)\n",
    "        \n",
    "    return state\n",
    "  \n",
    "print (\"Sense Check: Min State: {} Max State: {} Num States: {}\".format(observation_to_state([-5,-5,-5,-5.5]), observation_to_state([5,5,5,5.5]),\n",
    "                                                          num_states))\n",
    "                                                          \n",
    "import time\n",
    "state_values = np.random.rand(num_states) * 0.1\n",
    "state_rewards = np.zeros((num_states))\n",
    "state_transition_probabilities = np.ones((num_states, num_states, num_actions)) / num_states\n",
    "state_transition_counters = np.zeros((num_states, num_states, num_actions))\n",
    "\n",
    "def pick_best_action(current_state, state_values, state_transition_probabilities):\n",
    "    best_action = -1\n",
    "    best_action_value = -np.Inf\n",
    "    for a_i in range(num_actions):\n",
    "        action_value = state_transition_probabilities[current_state,:,a_i].dot(state_values)\n",
    "        if (action_value > best_action_value):\n",
    "            best_action_value = action_value\n",
    "            best_action = a_i\n",
    "        elif (action_value == best_action_value):\n",
    "            if np.random.randint(0,2) == 0:\n",
    "                best_action = a_i\n",
    "            \n",
    "    return best_action\n",
    "\n",
    "\n",
    "def update_state_transition_probabilities_from_counters(probabilities, counters):\n",
    "    for a_i in range(num_actions):\n",
    "        for s_i in range(num_states):\n",
    "            total_transitions_out_of_state = np.sum(counters[s_i,:,a_i])\n",
    "            if(total_transitions_out_of_state > 0):\n",
    "                probabilities[s_i,:,a_i] = counters[s_i,:,a_i] / total_transitions_out_of_state\n",
    "            \n",
    "    return probabilities\n",
    "\n",
    "\n",
    "def run_value_iteration(state_values, state_transition_probabilities, state_rewards):\n",
    "    start=time.time()\n",
    "    gamma = 0.995\n",
    "    convergence_tolerance = 0.01\n",
    "    iteration = 0\n",
    "    max_dif = np.Inf\n",
    "    \n",
    "    while max_dif > convergence_tolerance:  \n",
    "        iteration = iteration + 1\n",
    "        old_state_values = np.copy(state_values)\n",
    "\n",
    "        best_action_values = np.zeros((num_states)) - np.Inf\n",
    "        for a_i in range(num_actions):\n",
    "            best_action_values = \\\n",
    "                np.maximum(best_action_values, state_transition_probabilities[:,:,a_i].dot(state_values))\n",
    "\n",
    "        state_values = state_rewards + gamma * best_action_values\n",
    "        max_dif = np.max(np.abs(state_values - old_state_values))  \n",
    "    end=time.time()\n",
    "\n",
    "    \n",
    "    return state_values\n",
    "episode_rewards = []\n",
    "#env.monitor.start('training_dir3', force=True)\n",
    "for i_episode in range(150):\n",
    "    current_observation = env.reset()\n",
    "    current_state = observation_to_state(current_observation)\n",
    "    \n",
    "    episode_reward = 0\n",
    "    \n",
    "    for t in range(1000):\n",
    "        action = pick_best_action(current_state, state_values, state_transition_probabilities)\n",
    "        #print(\"action:\",action)\n",
    "        old_state = current_state\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        current_state = observation_to_state(observation)\n",
    "         \n",
    "        state_transition_counters[old_state, current_state, action] = \\\n",
    "            state_transition_counters[old_state, current_state, action] + 1\n",
    "        \n",
    "        episode_reward = episode_reward + reward        \n",
    "        \n",
    "        if done:\n",
    "            episode_rewards.append(episode_reward)\n",
    "            print (\"Reward: {}, Average reward over {} trials: {}\".format(episode_reward, i_episode, np.mean(episode_rewards[-100:]))      )      \n",
    "            \n",
    "            if(t < 195):\n",
    "                reward = -1\n",
    "            else:\n",
    "                reward = 0\n",
    "            state_rewards[current_state] = reward\n",
    "\n",
    "            state_transition_probabilities = update_state_transition_probabilities_from_counters(state_transition_probabilities, state_transition_counters)\n",
    "            state_values = run_value_iteration(state_values, state_transition_probabilities, state_rewards)\n",
    "            break\n",
    "mean_rewards=np.empty_like(episode_rewards);\n",
    "for i in range(150):\n",
    "    mean_rewards[i]=np.mean(episode_rewards[0:i])\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(mean_rewards)\n",
    "plt.plot(episode_rewards)\n",
    "\n",
    "plt.xlabel(\"episode count\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend([\"reward\", \"average\"], loc =\"lower right\")\n",
    "\n",
    "plt.figure(figsize=(24,24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_policy_iteration(state_values, state_transition_probabilities, state_rewards,state_policy):\n",
    "    start=time.time()\n",
    "    gamma = 0.995\n",
    "    convergence_tolerance = 0.01\n",
    "    iteration = 0\n",
    "    max_dif = np.Inf\n",
    "    while(True):\n",
    "        iteration = iteration + 1\n",
    "        while max_dif > convergence_tolerance:  \n",
    "            \n",
    "            old_state_values = np.copy(state_values)\n",
    "\n",
    "            best_action_values = np.zeros((num_states)) - np.Inf\n",
    "            for a_i in range(num_actions):\n",
    "                best_action_values = \\\n",
    "                    np.maximum(best_action_values, state_transition_probabilities[:,:,a_i].dot(state_values))\n",
    "\n",
    "            state_values = state_rewards + gamma * best_action_values\n",
    "            max_dif = np.max(np.abs(state_values - old_state_values)) \n",
    "        \n",
    "        new_policies=improve_policies(state_rewards,state_values,gamma)\n",
    "        \n",
    "        print(new_policies)\n",
    "        if(np.array_equal(new_policies,state_policy)):\n",
    "            break;\n",
    "        else:\n",
    "            state_policy=new_policies\n",
    "    \n",
    "    end=time.time()\n",
    "    print(\"policy iteration:\")\n",
    "    print(\"iter count:\",iteration)\n",
    "    print(\"time:\",end-start,\"s\")\n",
    "    return state_policy,state_values;\n",
    "\n",
    "def improve_policies(state_rewards,state_values,gamma):\n",
    "    new_state_policy=np.zeros((num_states),dtype=int)\n",
    "    for s_i in range(num_states):\n",
    "        rate_0=state_rewards[s_i];\n",
    "        rate_1=state_rewards[s_i];\n",
    "        for s_j in range(num_states):\n",
    "            rate_0+=gamma*state_transition_probabilities[s_i,s_j,0]*state_values[s_j]\n",
    "            rate_1+=gamma*state_transition_probabilities[s_i,s_j,1]*state_values[s_j]\n",
    "        if(rate_0>=rate_1):\n",
    "            new_state_policy[s_i]=0\n",
    "        else:\n",
    "            new_state_policy[s_i]=1\n",
    "    return new_state_policy\n",
    "import time\n",
    "state_values = np.random.rand(num_states) * 0.1\n",
    "state_rewards = np.zeros((num_states))\n",
    "state_transition_probabilities = np.ones((num_states, num_states, num_actions)) / num_states\n",
    "state_transition_counters = np.zeros((num_states, num_states, num_actions))\n",
    "\n",
    "state_policy=np.zeros((num_states),dtype=int)\n",
    "\n",
    "\n",
    "\n",
    "episode_rewards = []\n",
    "#env.monitor.start('training_dir3', force=True)\n",
    "for i_episode in range(50):\n",
    "    current_observation = env.reset()\n",
    "    current_state = observation_to_state(current_observation)\n",
    "    \n",
    "    episode_reward = 0\n",
    "    \n",
    "    for t in range(1000):\n",
    "        action = state_policy[current_state]\n",
    "        \n",
    "        old_state = current_state\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        current_state = observation_to_state(observation)\n",
    "         \n",
    "        state_transition_counters[old_state, current_state, action] = \\\n",
    "            state_transition_counters[old_state, current_state, action] + 1\n",
    "        \n",
    "        episode_reward = episode_reward + reward        \n",
    "        \n",
    "        if done:\n",
    "            episode_rewards.append(episode_reward)\n",
    "            print (\"Reward: {}, Average reward over {} trials: {}\".format(episode_reward, i_episode, np.mean(episode_rewards[-100:]))      )      \n",
    "            \n",
    "            if(t < 195):\n",
    "                reward = -1\n",
    "            else:\n",
    "                reward = 0\n",
    "            state_rewards[current_state] = reward\n",
    "\n",
    "            state_transition_probabilities = update_state_transition_probabilities_from_counters(state_transition_probabilities, state_transition_counters)\n",
    "            state_policy,state_values = run_policy_iteration(state_values, state_transition_probabilities, state_rewards,state_policy)\n",
    "            \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time, pickle, os\n",
    "\n",
    "env = gym.make('FrozenLake-v1')\n",
    "\n",
    "epsilon = 0.9\n",
    "total_episodes = 10000\n",
    "max_steps = 10000\n",
    "\n",
    "lr_rate = 0.81\n",
    "gamma = 0.99\n",
    "\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "def choose_action(state):\n",
    "    action=0\n",
    "    #if np.random.uniform(0, 1) < epsilon:\n",
    "     #   action = env.action_space.sample()\n",
    "    #else:\n",
    "    action = np.argmax(Q[state, :])\n",
    "    return action\n",
    "\n",
    "def learn(state, state2, reward, action):\n",
    "    predict = Q[state, action]\n",
    "    target = reward + gamma * np.max(Q[state2, :])\n",
    "    Q[state, action] = Q[state, action] + lr_rate * (target - predict)\n",
    "\n",
    "# Start\n",
    "for episode in range(total_episodes):\n",
    "    print(\"episode:\",episode)\n",
    "    state = env.reset()\n",
    "    t = 0\n",
    "    \n",
    "    while t < max_steps:\n",
    "        #env.render()\n",
    "\n",
    "        action = choose_action(state)  \n",
    "\n",
    "        state2, reward, done, info = env.step(action)  \n",
    "\n",
    "        learn(state, state2, reward, action)\n",
    "\n",
    "        state = state2\n",
    "\n",
    "        t += 1\n",
    "       \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "print(Q)\n",
    "\n",
    "with open(\"frozenLake_qTable.pkl\", 'wb') as f:\n",
    "    pickle.dump(Q, f)\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle, os\n",
    "\n",
    "env = gym.make('FrozenLake-v1')\n",
    "\n",
    "with open(\"frozenLake_qTable.pkl\", 'rb') as f:\n",
    "    Q = pickle.load(f)\n",
    "\n",
    "print(Q)\n",
    "def choose_action(state):\n",
    "    action = np.argmax(Q[state, :])\n",
    "    return action\n",
    "\n",
    "# start\n",
    "e=0\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    print(\"*** Episode: \", episode)\n",
    "    t = 0\n",
    "    while t < 100:\n",
    "        env.render()\n",
    "        action = choose_action(state)  \n",
    "        state2, reward, done, info = env.step(action)\n",
    "        state = state2\n",
    "        t=t+1\n",
    "        if done:\n",
    "            if(reward==1):\n",
    "                e+=1\n",
    "            print(t)\n",
    "            break\n",
    "        time.sleep(0.5)\n",
    "        os.system('clear')\n",
    "print(\"Q learning achives \"+str(e)+\" out of 100 episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLake-v1',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': True},\n",
    ")\n",
    "\n",
    "env = gym.make('FrozenLake-v1')\n",
    "\n",
    "# Instantiate the Environment.\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "\n",
    "# To check all environments present in OpenAI\n",
    "# print(envs.registry.all())\n",
    "# Total number of States and Actions\n",
    "number_of_states = env.observation_space.n\n",
    "number_of_actions = env.action_space.n\n",
    "print( \"States = \", number_of_states)\n",
    "print( \"Actions = \", number_of_actions)\n",
    "\n",
    "num_episodes = 10000\n",
    "steps_total = []\n",
    "rewards_total = []\n",
    "egreedy_total = []\n",
    "# PARAMS \n",
    "\n",
    "# Discount on reward\n",
    "gamma = 0.99\n",
    "\n",
    "# Factor to balance the ratio of action taken based on past experience to current situtation\n",
    "learning_rate = 0.9\n",
    "egreedy = 0.7\n",
    "egreedy_final = 0.1\n",
    "egreedy_decay = 0.999\n",
    "Q = torch.zeros([number_of_states, number_of_actions])\n",
    "import time\n",
    "start=time.time()\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    \n",
    "    # resets the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "\n",
    "        if random_for_egreedy > egreedy:      \n",
    "            random_values = Q[state] + torch.rand(1,number_of_actions) / 1000      \n",
    "            action = torch.max(random_values,1)[1][0]  \n",
    "            action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        if egreedy > egreedy_final:\n",
    "            egreedy *= egreedy_decay\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Filling the Q Table\n",
    "        Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "        \n",
    "        # Setting new state for next action\n",
    "        state = new_state\n",
    "        \n",
    "        # env.render()\n",
    "        # time.sleep(0.4)\n",
    "        \n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(reward)\n",
    "            egreedy_total.append(egreedy)\n",
    "            if i_episode % 10 == 0:\n",
    "                print('Episode: {} Reward: {} Steps Taken: {}'.format(i_episode,reward, step))\n",
    "            break\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "print(Q)\n",
    "def choose_action(state):\n",
    "    action = np.argmax(Q[state, :])\n",
    "    return action\n",
    "\n",
    "# start\n",
    "e=0\n",
    "for episode in range(100):\n",
    "    state = env.reset()\n",
    "    print(\"*** Episode: \", episode)\n",
    "    t = 0\n",
    "    while t < 100:\n",
    "        \n",
    "        random_values = Q[state] + torch.rand(1,number_of_actions) / 1000      \n",
    "        action = torch.max(random_values,1)[1][0]  \n",
    "        action = action.item()\n",
    "        state2, reward, done, info = env.step(action)\n",
    "        state = state2\n",
    "        t=t+1\n",
    "        if done:\n",
    "            if(reward==1):\n",
    "                e+=1\n",
    "            print(t)\n",
    "            break\n",
    "        \n",
    "print(\"Q learning achives \"+str(e)+\" out of 100 episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\"\"\"\n",
    "Base code taken from: \n",
    "https://github.com/IsaacPatole/CartPole-v0-using-Q-learning-SARSA-and-DNN/blob/master/Qlearning_for_cartpole.py\n",
    "\"\"\"\n",
    "\n",
    "class CartPoleQAgent():\n",
    "    def __init__(self, buckets=(7, 7, 7, 7), \n",
    "                 num_episodes=10000, min_lr=0.1, \n",
    "                 min_epsilon=0.01, discount=0.995, decay=25):\n",
    "        self.buckets = buckets\n",
    "        self.num_episodes = num_episodes\n",
    "        self.min_lr = min_lr\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.discount = discount\n",
    "        self.decay = decay\n",
    "\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        \n",
    "        # This is the action-value function being initialized to 0's\n",
    "        self.Q_table = np.zeros(self.buckets + (self.env.action_space.n,))\n",
    "\n",
    "        # [position, velocity, angle, angular velocity]\n",
    "        self.upper_bounds = [self.env.observation_space.high[0], 0.5, self.env.observation_space.high[2], math.radians(50) / 1.]\n",
    "        self.lower_bounds = [self.env.observation_space.low[0], -0.5, self.env.observation_space.low[2], -math.radians(50) / 1.]\n",
    "        \n",
    "        #\n",
    "        self.steps = np.zeros(self.num_episodes)\n",
    "        \n",
    "        \n",
    "\n",
    "    def discretize_state(self, obs):\n",
    "        \"\"\"\n",
    "        Takes an observation of the environment and aliases it.\n",
    "        By doing this, very similar observations can be treated\n",
    "        as the same and it reduces the state space so that the \n",
    "        Q-table can be smaller and more easily filled.\n",
    "        \n",
    "        Input:\n",
    "        obs (tuple): Tuple containing 4 floats describing the current\n",
    "                     state of the environment.\n",
    "        \n",
    "        Output:\n",
    "        discretized (tuple): Tuple containing 4 non-negative integers smaller \n",
    "                             than n where n is the number in the same position\n",
    "                             in the buckets list.\n",
    "        \"\"\"\n",
    "        discretized = list()\n",
    "        for i in range(len(obs)):\n",
    "            scaling = ((obs[i] + abs(self.lower_bounds[i])) \n",
    "                       / (self.upper_bounds[i] - self.lower_bounds[i]))\n",
    "            new_obs = int(round((self.buckets[i] - 1) * scaling))\n",
    "            new_obs = min(self.buckets[i] - 1, max(0, new_obs))\n",
    "            discretized.append(new_obs)\n",
    "        return tuple(discretized)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Implementation of e-greedy algorithm. Returns an action (0 or 1).\n",
    "        \n",
    "        Input:\n",
    "        state (tuple): Tuple containing 4 non-negative integers within\n",
    "                       the range of the buckets.\n",
    "        \n",
    "        Output:\n",
    "        (int) Returns either 0 or 1\n",
    "        \"\"\"\n",
    "        if (np.random.random() < self.epsilon):\n",
    "            return self.env.action_space.sample() \n",
    "        else:\n",
    "            return np.argmax(self.Q_table[state])\n",
    "        \n",
    "    def get_action(self, state, e):\n",
    "        \"\"\"\n",
    "        Another policy based on the Q-table. Slight variation from \n",
    "        e-greedy. It assumes the state fed hasn't been discretized and \n",
    "        returns a vector with probabilities for each action.\n",
    "        \n",
    "        Input: \n",
    "        state (tuple): Contains the 4 floats used to describe\n",
    "                       the current state of the environment.\n",
    "        e (int): Denotes the episode at which the agent is supposed\n",
    "                 to be, helping balance exploration and exploitation.\n",
    "                 \n",
    "        Output:\n",
    "        action_vector (numpy array): Vector containing the probability\n",
    "                                     of each action being chosen at the\n",
    "                                     current state.\n",
    "        \"\"\"\n",
    "        obs = self.discretize_state(state)\n",
    "        action_vector = self.Q_table[obs]\n",
    "        epsilon = self.get_epsilon(e)\n",
    "        action_vector = self.normalize(action_vector, epsilon)\n",
    "        return action_vector\n",
    "\n",
    "    def normalize(self, action_vector, epsilon):\n",
    "        \"\"\"\n",
    "        Returns a vector with components adding to 1. Ensures \n",
    "        \n",
    "        Input:\n",
    "        action_vector (numpy array): Contains expected values for each\n",
    "                                     action at current state from Q-table.\n",
    "        epsilon (float): Chances that the e-greedy algorithm would \n",
    "                         choose an action at random. With this pol\n",
    "        \n",
    "        Output:\n",
    "        new_vector (numpy array): Vector containing the probability\n",
    "                                  of each action being chosen at the\n",
    "                                  current state.\n",
    "        \"\"\"\n",
    "        \n",
    "        total = sum(action_vector)\n",
    "        new_vector = (1-epsilon)*action_vector/(total)\n",
    "        new_vector += epsilon/2.0\n",
    "        return new_vector\n",
    "\n",
    "    def update_q(self, state, action, reward, new_state):\n",
    "        \"\"\"\n",
    "        Updates Q-table using the rule as described by Sutton and Barto in\n",
    "        Reinforcement Learning.\n",
    "        \"\"\"\n",
    "        self.Q_table[state][action] += (self.learning_rate * \n",
    "                                        (reward \n",
    "                                         + self.discount * np.max(self.Q_table[new_state]) \n",
    "                                         - self.Q_table[state][action]))\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        \"\"\"Gets value for epsilon. It declines as we advance in episodes.\"\"\"\n",
    "        # Ensures that there's almost at least a min_epsilon chance of randomly exploring\n",
    "        return max(self.min_epsilon, min(1., 1. - math.log10((t + 1) / self.decay)))\n",
    "\n",
    "    def get_learning_rate(self, t):\n",
    "        \"\"\"Gets value for learning rate. It declines as we advance in episodes.\"\"\"\n",
    "        # Learning rate also declines as we add more episodes\n",
    "        return max(self.min_lr, min(1., 1. - math.log10((t + 1) / self.decay)))\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains agent making it go through the environment and choose actions\n",
    "        through an e-greedy policy and updating values for its Q-table. The \n",
    "        agent is trained by default for 500 episodes with a declining \n",
    "        learning rate and epsilon values that with the default values,\n",
    "        reach the minimum after 198 episodes.\n",
    "        \"\"\"\n",
    "        # Looping for each episode\n",
    "        for e in range(self.num_episodes):\n",
    "            # Initializes the state\n",
    "            current_state = self.discretize_state(self.env.reset())\n",
    "\n",
    "            self.learning_rate = self.get_learning_rate(e)\n",
    "            self.epsilon = self.get_epsilon(e)\n",
    "            done = False\n",
    "            \n",
    "            # Looping for each step\n",
    "            while not done:\n",
    "                self.steps[e] += 1\n",
    "                # Choose A from S\n",
    "                action = self.choose_action(current_state)\n",
    "                # Take action\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                new_state = self.discretize_state(obs)\n",
    "                # Update Q(S,A)\n",
    "                self.update_q(current_state, action, reward, new_state)\n",
    "                current_state = new_state\n",
    "                \n",
    "                # We break out of the loop when done is False which is\n",
    "                # a terminal state.\n",
    "        print('Finished training!')\n",
    "    \n",
    "    def plot_learning(self):\n",
    "        \"\"\"\n",
    "        Plots the number of steps at each episode and prints the\n",
    "        amount of times that an episode was successfully completed.\n",
    "        \"\"\"\n",
    "        mean_rewards=np.empty_like(self.steps);\n",
    "        n=len(self.steps);\n",
    "        for i in range(n):\n",
    "            mean_rewards[i]=np.mean(self.steps[0:i])\n",
    "        plt.figure()\n",
    "        plt.plot(mean_rewards)\n",
    "        plt.plot(self.steps)\n",
    "        plt.xlabel(\"episode count\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.legend([\"reward\", \"average\"], loc =\"lower right\")\n",
    "        plt.figure(figsize=(24,24))\n",
    "        t = 0\n",
    "        for i in range(self.num_episodes):\n",
    "            if self.steps[i] == 200:\n",
    "                t+=1\n",
    "        print(t, \"episodes were successfully completed.\",mean_rewards[-1])\n",
    "        \n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Runs an episode while displaying the cartpole environment.\"\"\"\n",
    "        self.env = gym.wrappers.Monitor(self.env,'cartpole')\n",
    "        t = 0\n",
    "        done = False\n",
    "        current_state = self.discretize_state(self.env.reset())\n",
    "        while not done:\n",
    "                self.env.render()\n",
    "                t = t+1\n",
    "                action = self.choose_action(current_state)\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                new_state = self.discretize_state(obs)\n",
    "                current_state = new_state\n",
    "            \n",
    "        return t   \n",
    "def load_q_learning():\n",
    "    agent = CartPoleQAgent()\n",
    "    agent.train()\n",
    "    agent.plot_learning()\n",
    "\n",
    "    return agent\n",
    "start=time.time()\n",
    "agent = load_q_learning()\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
